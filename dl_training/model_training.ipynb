{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2zBtY3Xa0KeY"
   },
   "source": [
    "## Text classification using BERT pre-trained models\n",
    "The current code was adapted from this TF website: https://www.tensorflow.org/text/tutorials/classify_text_with_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GyKjMfe40Keb"
   },
   "outputs": [],
   "source": [
    "!pip install -q -U tensorflow\n",
    "!pip install -q -U tensorflow-text\n",
    "!pip install -q tf-models-official"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3kehSwMd0Kec"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yec2rAsH0Ked"
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "from pathlib import Path\n",
    "\n",
    "#Download JSON file if not present\n",
    "if not Path(\"dataset/News_Category_Dataset_v2.json\").exists():\n",
    "    BUCKET_NAME = 'factored-sandbox'\n",
    "    KEY = 'david-avila/News_Category_Dataset_v2.json'\n",
    "    s3 = boto3.resource('s3')\n",
    "\n",
    "    try:\n",
    "        s3.Bucket(BUCKET_NAME).download_file(KEY, \"dataset/News_Category_Dataset_v2.json\")\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        if e.response['Error']['Code'] == \"404\":\n",
    "            print(\"The object does not exist.\")\n",
    "        else:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3p2LSwO0Ked"
   },
   "outputs": [],
   "source": [
    "#Load dataset from JSON file\n",
    "data_inputs = []\n",
    "data_labels = []\n",
    "with open(\"dataset/News_Category_Dataset_v2.json\", \"r\") as f:\n",
    "    for line in f:\n",
    "        article = json.loads(line)\n",
    "        data_inputs.append(article[\"headline\"])\n",
    "        data_labels.append(article[\"category\"])\n",
    "\n",
    "all_labels = np.array(list(set(data_labels)))\n",
    "num_topics = all_labels.shape[0]\n",
    "data_inputs = np.array(data_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qdRURg530Ked"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "#Create one-hot encoding for the labels\n",
    "#1. Encode each label (topic) into num type\n",
    "#Label_encoder object knows how to understand word labels\n",
    "label_encoder = LabelEncoder()\n",
    "#Encode labels\n",
    "numeric_labels = label_encoder.fit_transform(data_labels)\n",
    "\n",
    "#2. Get one-hot encoding of each label\n",
    "onehotencoder = OneHotEncoder()\n",
    "#Reshape the 1-D label array to 2-D, as fit_transform expects 2-D, and fit the array \n",
    "numeric_labels = onehotencoder.fit_transform(numeric_labels.reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V4zXMIKs0Kee"
   },
   "outputs": [],
   "source": [
    "#Split dataset to get train, test and validation splits (80-10-10)\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_inputs, numeric_labels, test_size=0.1, random_state=1)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.111, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SPSaDRsa0Kef"
   },
   "outputs": [],
   "source": [
    "#Select BERT model to use and fine-tune\n",
    "tfhub_handle_encoder = \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\"\n",
    "tfhub_handle_preprocess = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HiK3dU160Keg"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dropout, Dense\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "#Load pre-processing model and main BERT model as Keras Layers\n",
    "#TODO: Adjust the last layer's activation if needed to classify (softmax maybe needed)\n",
    "bert_preprocess_layer = hub.KerasLayer(tfhub_handle_preprocess, name=\"preprocessing\")\n",
    "bert_encoder_layer = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name=\"BERT_encoder\")\n",
    "#Create classifier model to fine-tune BERT outputs\n",
    "def build_classifier_model():\n",
    "    text_input = Input(shape=(), dtype=tf.string, name='text')\n",
    "    encoder_inputs = bert_preprocess_layer(text_input)\n",
    "    outputs = bert_encoder_layer(encoder_inputs)\n",
    "    net = outputs['pooled_output']\n",
    "    net = Dropout(0.1)(net)\n",
    "    net = Dense(num_topics, activation=\"softmax\", name='classifier')(net)\n",
    "    return Model(text_input, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NP3yO7bl0Keh"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "#Define loss function\n",
    "loss = CategoricalCrossentropy()\n",
    "\n",
    "#Define epochs and optimizer as AdamW\n",
    "epochs = 40\n",
    "steps_per_epoch = len(x_train)\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "init_lr = 3e-5\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fGE7IZyF0Keh"
   },
   "outputs": [],
   "source": [
    "#Build and compile model with given loss and metrics needed\n",
    "classifier_model = build_classifier_model()\n",
    "classifier_model.compile(optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define callbacks to stop when 95% is reached and save weights\n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('accuracy')>0.95):\n",
    "            print(\"\\nReached 95% accuracy so cancelling training!\")\n",
    "            self.model.stop_training = True\n",
    "stop_cb = myCallback()\n",
    "            \n",
    "checkpoint_filepath = '/tmp/checkpoint'\n",
    "model_checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2-re_7EU0Kei",
    "outputId": "3d0080e1-39ea-47c2-8ca3-a366fff4f553"
   },
   "outputs": [],
   "source": [
    "#Fit model\n",
    "history = classifier_model.fit(x=x_train, y=y_train,\n",
    "                               validation_data=(x_val, y_val), \n",
    "                               callbacks=[stop_cb, model_checkpoint_cb],\n",
    "                               epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eiL8ph2M0Kej",
    "outputId": "73e15619-7697-4bcd-96d2-db71698db593"
   },
   "outputs": [],
   "source": [
    "predicted_class = np.argmax(classifier_model.predict([\"The politicians made it official: it's diplomatic football!\"]))\n",
    "print(label_encoder.inverse_transform([predicted_class]))\n",
    "predicted_class = np.argmax(classifier_model.predict([\"Football is all for people!\"]))\n",
    "print(label_encoder.inverse_transform([predicted_class]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Zj55wXHNjtZ",
    "outputId": "391bc914-b80f-4e77-f7f1-297757e8c92a"
   },
   "outputs": [],
   "source": [
    "print(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jg2ZlzsAQ0Vb",
    "outputId": "dc081de7-5cce-417c-8910-1e5b14cf17b5"
   },
   "outputs": [],
   "source": [
    "#Save and export model to load it within Java TF and label classes as JSON\n",
    "classifier_model.save(\"topics_classifier.h5\")\n",
    "dict_labels = dict(zip(range(0,num_topics), label_encoder.classes_))\n",
    "with open(\"topics.json\", \"w\") as f:\n",
    "    json.dump(dict_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "model_training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
